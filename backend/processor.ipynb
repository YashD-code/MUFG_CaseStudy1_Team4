{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd10a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api.py\n",
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from fastapi import FastAPI, UploadFile, File, Form\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import FileResponse\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Allow React frontend\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "os.makedirs(\"processed_files\", exist_ok=True)\n",
    "\n",
    "# ----------------------- FILE LOADER -----------------------\n",
    "def load_file(path):\n",
    "    ext = path.lower()\n",
    "    if ext.endswith(\".xlsx\") or ext.endswith(\".xls\"):\n",
    "        xls = pd.ExcelFile(path)\n",
    "        return {sheet: xls.parse(sheet) for sheet in xls.sheet_names}\n",
    "    elif ext.endswith(\".csv\"):\n",
    "        try:\n",
    "            return {\"Sheet1\": pd.read_csv(path, encoding=\"utf-8\")}\n",
    "        except UnicodeDecodeError:\n",
    "            return {\"Sheet1\": pd.read_csv(path, encoding=\"latin1\")}\n",
    "    else:\n",
    "        raise Exception(\"Unsupported file format\")\n",
    "\n",
    "# ----------------------- NAN CLEANER -----------------------\n",
    "def df_to_json_safe(df):\n",
    "    \"\"\"Convert DataFrame to JSON-safe dict (NaN â†’ None)\"\"\"\n",
    "    return df.where(pd.notnull(df), None).to_dict(orient=\"records\")\n",
    "\n",
    "# ----------------------- DATA OPERATIONS -----------------------\n",
    "def handle_missing(df, strategy=\"drop\", numeric_strategy=\"mean\"):\n",
    "    df2 = df.copy()\n",
    "    for col in df2.columns:\n",
    "        if df2[col].dtype in [\"float64\", \"int64\"]:\n",
    "            if strategy == \"drop\":\n",
    "                df2 = df2[df2[col].notna()]\n",
    "            elif strategy == \"impute\":\n",
    "                if numeric_strategy==\"mean\":\n",
    "                    df2[col] = df2[col].fillna(df2[col].mean())\n",
    "                elif numeric_strategy==\"median\":\n",
    "                    df2[col] = df2[col].fillna(df2[col].median())\n",
    "        else:\n",
    "            df2[col] = df2[col].fillna(\"missing\")\n",
    "    return df2\n",
    "\n",
    "def remove_outliers(df, params={}):\n",
    "    cols = params.get(\"columns\", df.select_dtypes(include=['number']).columns)\n",
    "    df2 = df.copy()\n",
    "    for col in cols:\n",
    "        if col not in df2.columns: continue\n",
    "        Q1 = df2[col].quantile(0.25)\n",
    "        Q3 = df2[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        df2 = df2[(df2[col] >= Q1 - 1.5*IQR) & (df2[col] <= Q3 + 1.5*IQR)]\n",
    "    return df2\n",
    "\n",
    "def remove_duplicates(df, params={}):\n",
    "    subset = params.get(\"columns\")\n",
    "    df2 = df.copy()\n",
    "    return df2.drop_duplicates(subset=subset, keep=False).reset_index(drop=True)  # remove all duplicates\n",
    "\n",
    "def replace_values(df, params={}):\n",
    "    find = params.get(\"find\")\n",
    "    replace = params.get(\"replace\")\n",
    "    if find is None:\n",
    "        return df\n",
    "    return df.replace(find, replace)\n",
    "\n",
    "def normalize_text(df, params={}):\n",
    "    cols = params.get(\"columns\", df.columns)\n",
    "    mode = params.get(\"mode\", \"lower\")\n",
    "    df2 = df.copy()\n",
    "    for col in cols:\n",
    "        if col not in df2.columns: continue\n",
    "        df2[col] = df2[col].astype(str).str.strip()\n",
    "        if mode==\"lower\": df2[col] = df2[col].str.lower()\n",
    "        elif mode==\"upper\": df2[col] = df2[col].str.upper()\n",
    "    return df2\n",
    "\n",
    "def filter_rows(df, params={}):\n",
    "    col = params.get(\"column\")\n",
    "    cond = params.get(\"condition\")\n",
    "    value = params.get(\"value\")\n",
    "    if col not in df.columns: return df\n",
    "    series = df[col]\n",
    "    try:\n",
    "        if cond==\"equals\": return df[series==value]\n",
    "        if cond==\"contains\": return df[series.astype(str).str.contains(str(value), na=False)]\n",
    "        if cond==\"greater_than\": return df[pd.to_numeric(series, errors=\"coerce\") > float(value)]\n",
    "        if cond==\"less_than\": return df[pd.to_numeric(series, errors=\"coerce\") < float(value)]\n",
    "    except: return df\n",
    "    return df\n",
    "\n",
    "def merge_columns(df, params={}):\n",
    "    cols = params.get(\"columns\")\n",
    "    new_col = params.get(\"new_column\", \"merged_column\")\n",
    "    sep = params.get(\"separator\", \" \")\n",
    "    if not cols or any(col not in df.columns for col in cols): return df\n",
    "    df2 = df.copy()\n",
    "    df2[new_col] = df2[cols].astype(str).agg(sep.join, axis=1)\n",
    "    return df2\n",
    "\n",
    "def convert_data_types(df, params={}):\n",
    "    col = params.get(\"column\")\n",
    "    typ = params.get(\"new_type\")\n",
    "    if col not in df.columns: return df\n",
    "    df2 = df.copy()\n",
    "    try:\n",
    "        if typ==\"int\": df2[col]=pd.to_numeric(df2[col], errors=\"coerce\").astype(\"Int64\")\n",
    "        elif typ==\"float\": df2[col]=pd.to_numeric(df2[col], errors=\"coerce\")\n",
    "        elif typ==\"datetime\": df2[col]=pd.to_datetime(df2[col], errors=\"coerce\")\n",
    "        elif typ==\"string\": df2[col]=df2[col].astype(str)\n",
    "    except: pass\n",
    "    return df2\n",
    "\n",
    "# Map operations\n",
    "OPERATION_FUNCTIONS = {\n",
    "    \"handle_missing\": handle_missing,\n",
    "    \"remove_duplicates\": remove_duplicates,\n",
    "    \"replace_values\": replace_values,\n",
    "    \"normalize_text\": normalize_text,\n",
    "    \"filter_rows\": filter_rows,\n",
    "    \"merge_columns\": merge_columns,\n",
    "    \"convert_data_types\": convert_data_types,\n",
    "    \"remove_outliers\": remove_outliers\n",
    "}\n",
    "\n",
    "# ----------------------- PROCESS FILE -----------------------\n",
    "def process_file(file_path, operations, priority_ops):\n",
    "    sheets = load_file(file_path)\n",
    "    ordered_ops = priority_ops + [op for op in operations if op not in priority_ops]\n",
    "    output = {}\n",
    "    stats = {}\n",
    "\n",
    "    for sheet_name, df in sheets.items():\n",
    "        df_clean = df.copy()\n",
    "        initial_rows = len(df_clean)\n",
    "        initial_missing = df_clean.isna().sum().sum()\n",
    "\n",
    "        for op in ordered_ops:\n",
    "            func = OPERATION_FUNCTIONS.get(op)\n",
    "            if not func: continue\n",
    "            params = operations.get(op, {})\n",
    "            df_clean = func(df_clean, params)\n",
    "\n",
    "        duplicates_removed = initial_rows - len(df_clean)\n",
    "        missing_after = df_clean.isna().sum().sum()\n",
    "\n",
    "        output[sheet_name] = df_clean\n",
    "        stats[sheet_name] = {\n",
    "            \"initial_rows\": initial_rows,\n",
    "            \"duplicates_removed\": duplicates_removed,\n",
    "            \"initial_missing\": initial_missing,\n",
    "            \"missing_after\": missing_after,\n",
    "            \"operations_applied\": ordered_ops\n",
    "        }\n",
    "\n",
    "    first_sheet = list(output.keys())[0]\n",
    "\n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"message\": \"Processed successfully\",\n",
    "        \"data\": output,\n",
    "        \"preview\": df_to_json_safe(output[first_sheet]),\n",
    "        \"sheet_names\": list(output.keys()),\n",
    "        \"stats\": stats\n",
    "    }\n",
    "\n",
    "# ----------------------- API ENDPOINTS -----------------------\n",
    "@app.post(\"/process\")\n",
    "async def process_file_api(\n",
    "    file: UploadFile = File(...),\n",
    "    operations: str = Form(...),\n",
    "    priority_ops: str = Form(...)\n",
    "):\n",
    "    # Save uploaded file\n",
    "    suffix = os.path.splitext(file.filename)[1]\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as temp:\n",
    "        temp.write(await file.read())\n",
    "        temp.flush()\n",
    "        temp_path = temp.name\n",
    "\n",
    "    # Run processor\n",
    "    result = process_file(\n",
    "        temp_path,\n",
    "        json.loads(operations),\n",
    "        json.loads(priority_ops)\n",
    "    )\n",
    "\n",
    "    # Save output file\n",
    "    file_id = f\"{uuid.uuid4()}.xlsx\"\n",
    "    output_path = f\"processed_files/{file_id}\"\n",
    "    first_sheet = list(result[\"data\"].keys())[0]\n",
    "    result[\"data\"][first_sheet].to_excel(output_path, index=False)\n",
    "\n",
    "    return {\n",
    "        \"status\": \"completed\",\n",
    "        \"file_id\": file_id,\n",
    "        \"sheet\": first_sheet,\n",
    "        \"preview\": result[\"preview\"],\n",
    "        \"operations_performed\": list(json.loads(operations).keys())\n",
    "    }\n",
    "\n",
    "@app.get(\"/history\")\n",
    "def get_history():\n",
    "    files = []\n",
    "    for f in os.listdir(\"processed_files\"):\n",
    "        if f.endswith(\".xlsx\"):\n",
    "            files.append({\n",
    "                \"id\": f,\n",
    "                \"name\": f,\n",
    "                \"created_at\": os.path.getmtime(f\"processed_files/{f}\")\n",
    "            })\n",
    "    return files\n",
    "\n",
    "@app.get(\"/download/{file_id}\")\n",
    "def download(file_id: str):\n",
    "    return FileResponse(f\"processed_files/{file_id}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
